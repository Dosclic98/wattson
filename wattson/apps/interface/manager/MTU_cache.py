from copy import deepcopy
from dataclasses import dataclass, field
import time
from typing import Iterable, Dict, Optional, Union, List
import threading as th


from wattson.iec104.interface.types import TypeID
from wattson.apps.interface.util.messages import IECMsg
from wattson.apps.interface.util.msg_status import MsgStatus


@dataclass
class CacheEntry:
    msg: IECMsg
    status: MsgStatus = MsgStatus.WAITING_FOR_SEND
    ts: int = 0
    IO_cache: Dict[int, Union[int, bool, float]] = field(default_factory=dict)

    def set_confirmed(self):
        self.status = MsgStatus.RECEIVED_ACK
        self.ts = int(time.time())

    def is_cache_filled(self):
        msg_dict = self.msg.__dict__
        if "ioa" in msg_dict and self.msg.ioa in self.IO_cache:
            return True
        elif "val_map" in msg_dict:
            for ioa in self.msg.val_map:
                if ioa not in self.IO_cache:
                    return False
        return True

    def __repr__(self):
        s = f"CacheEntry with msg: {self.msg} in status {self.status.__repr__()}"
        if self.IO_cache:
            return s + f" and IOs {self.IO_cache}"
        return s

    def decrement_msg_max_tries(self):
        if self.msg.max_tries == 0:
            raise RuntimeError(f"Cannot decrement max-tries, already at 0: {self}")
        self.msg.max_tries -= 1


class CacheOverwriteError(RuntimeError):
    pass


class IllegalCacheAccessError(RuntimeError):
    pass


class MessageCache:
    """
    The Message cache stores information about changes to be applied,
        in action, or finished - if final messages such as ACTTERMs are optional
    Both, APDUs generated by the MTU, and on request of subscribers are stored.

    This is necessary, because IEC-104 traffic should NOT be simply duplicated
        and send to all the subribers but the behaviour of the network and all
        relevant information still published in an easily filterable way.
    Thus, only the first message identifying the outgoing IEC-104 packets
        is included fully during publishing, while all those later will
        be of the 'Confirmation', including the first message's reference number.

    Further, this makes clear and which changes are (supposed) to be applied
        next, even if multiple subscribers send over controls, while the MTU also
        has its own static list of commands necessary to send.
    It also tries to prevent corruption by only allowing this single change
        per datapoint.

    Usage info:
        The total cache is divided three-fold:
            - global-coa-compatible cache (Sys-info-control type-IDs except for C_RD)
            - active-dps (type-IDs 45-64)
            - param-changes (type-IDs 110-113)
    """
    def __init__(self, rtu_coas: Iterable[int], dt_remove_after_confirmed: int):
        print(f'RTU COAs at start! {rtu_coas=}')
        self._active_dps: Dict[int, Dict[int, CacheEntry]] = {coa: {} for coa in rtu_coas}
        print(f"post-init coas: {self._active_dps=}")
        # active & queued global cache necessary due to subscription-autonomous sendings.
        # strategy: Only allow sending from queued global cache once ACTTERM from all
        # rtus have been received.
        self._active_global_per_rtu_cache: Dict[int, Dict[TypeID, CacheEntry]] = {
            coa: {} for coa in rtu_coas
        }
        self._queued_global_per_rtu_cache: Dict[int, Dict[TypeID, CacheEntry]] = {
            coa: {} for coa in rtu_coas
        }
        # only allow one type of change at the same time... please no more statekeeping
        self._active_param_changes: Dict[int, Dict[int, CacheEntry]] = {
            coa: {} for coa in rtu_coas
        }
        self._active_global_coa_cache: Dict[TypeID, CacheEntry] = {}
        self._queued_global_coa_cache: Dict[TypeID, CacheEntry] = {}
        self._confirmed_dps: Dict[int, Dict[int, CacheEntry]] = {coa: {} for coa in rtu_coas}

        self.dt_remove_after_confirmed = dt_remove_after_confirmed
        self.global_cache_lock = th.RLock()
        self.dp_cache_lock = th.RLock()
        self.param_cache_lock = th.RLock()

    def clean_cache_for_rtu(self, coa: int) -> List[str]:
        """
        Deletes all entries for this RTU (if no longer connected etc.)
        Only does so for the *active and queued* caches (or should I change this?)

        Args:
            coa: ID of RTU

        Returns:
            all reference numbers of msgs that are thereby "cancelled or for which an ACT_CON/TERM could have still come
                but is optional
        """
        ref_nrs = set()
        with self.dp_cache_lock:
            for e in self._active_dps[coa].values():
                ref_nrs.add(e.msg.reference_nr)
            for e in self._confirmed_dps[coa].values():
                ref_nrs.add(e.msg.reference_nr)

            self._active_dps[coa] = {}
            self._confirmed_dps[coa] = {}

        with self.param_cache_lock:
            for e in self._active_param_changes[coa].values():
                ref_nrs.add(e.msg.reference_nr)
            self._active_param_changes[coa] = {}

        with self.global_cache_lock:
            c = self._active_global_per_rtu_cache.pop(coa, {})
            for e in c.values():
                ref_nrs.add(e.msg.reference_nr)

            c = self._queued_global_per_rtu_cache.pop(coa, {})
            for e in c.values():
                ref_nrs.add(e.msg.reference_nr)

            self._active_global_per_rtu_cache[coa] = {}
            self._queued_global_per_rtu_cache[coa] = {}
        return list(sorted(ref_nrs))

    def get_entry_if_dp_is_active(self, coa: int, ioa: int) -> Optional[CacheEntry]:
        with self.dp_cache_lock:
            if not self.is_dp_active(coa, ioa):
                return None
            return self._active_dps[coa][ioa]

    def get_param_entry_if_active(self, coa: int, ioa: int) -> Optional[CacheEntry]:
        with self.param_cache_lock:
            # TODO: reactivate
            # empty cache
            assert coa in self._active_param_changes
            if ioa in self._active_param_changes[coa]:
                return self._active_param_changes[coa][ioa]
            return None

    def get_entry_if_send_and_nonterminated(self, coa: int, ioa: int) -> Optional[CacheEntry]:
        with self.dp_cache_lock:
            assert coa in self._active_dps
            if ioa in self._active_dps[coa] and self._active_dps[coa][ioa].status != MsgStatus.WAITING_FOR_SEND:
                return self._active_dps[coa][ioa]
            elif ioa in self._confirmed_dps[coa]:
                return self._confirmed_dps[coa][ioa]
            return None

    def get_queued_dp_entry_if_exists(self, coa: int, ioa: int) -> Optional[CacheEntry]:
        with self.dp_cache_lock:
            assert coa in self._active_dps
            if ioa in self._active_dps[coa]:
                return self._active_dps[coa][ioa]
            return None

    def get_entry_if_dp_is_confirmed(self, coa: int, ioa: int) -> Optional[CacheEntry]:
        with self.dp_cache_lock:
            assert coa in self._confirmed_dps
            if ioa in self._confirmed_dps[coa]:
                return self._confirmed_dps[coa][ioa]
            return None

    def get_global_entry_if_is_active(self, coa: int, type_ID: TypeID) -> Optional[CacheEntry]:
        with self.global_cache_lock:
            if type_ID in self._active_global_per_rtu_cache[coa]:
                return self._active_global_per_rtu_cache[coa][type_ID]
            return None

    def get_global_coa_entry_if_is_active(self, type_ID: TypeID) -> Optional[CacheEntry]:
        with self.global_cache_lock:
            if type_ID in self._active_global_coa_cache:
                return self._active_global_coa_cache[type_ID]
            return None

    def get_global_coa_entry_if_is_queued(self, type_ID: TypeID) -> Optional[CacheEntry]:
        with self.global_cache_lock:
            if type_ID in self._queued_global_coa_cache:
                return self._queued_global_coa_cache[type_ID]
            return None

    def get_queued_global_if_is_active(self, coa: int, type_ID: TypeID) -> Optional[CacheEntry]:
        with self.global_cache_lock:
            assert coa in self._queued_global_per_rtu_cache
            if type_ID in self._queued_global_per_rtu_cache[coa]:
                return self._queued_global_per_rtu_cache[coa][type_ID]
            return None

    def pop_queued_global_if_is_active(self, coa: int, type_ID: TypeID) -> Optional[CacheEntry]:
        with self.global_cache_lock:
            assert coa in self._queued_global_per_rtu_cache
            if type_ID in self._queued_global_per_rtu_cache[coa]:
                self._queued_global_per_rtu_cache[coa].pop(type_ID)
            return None

    def pop_active_msg(self, coa: int, ioa: int) -> IECMsg:
        with self.dp_cache_lock:
            return self._active_dps[coa].pop(ioa).msg

    def pop_archived_msg(self, coa: int, ioa: int) -> IECMsg:
        with self.dp_cache_lock:
            return self._confirmed_dps[coa].pop(ioa).msg

    def remove_active_entry(self, coa: int, ioa: int) -> None:
        _ = self.pop_active_entry(coa, ioa)

    def remove_archived_entry(self, coa: int, ioa: int) -> None:
        _ = self.pop_archived_msg(coa, ioa)

    def remove_param_entry(self, coa: int, ioa: int) -> None:
        # TODO: add to testing-routine
        _ = self.pop_param_mesage(coa, ioa)

    def pop_active_entry(self, coa: int, ioa: int) -> CacheEntry:
        with self.dp_cache_lock:
            return self._active_dps[coa].pop(ioa)

    def pop_global_entry(
            self, coa: int, type_ID: TypeID, remove_global_coa_if_done: bool = False
    ) -> CacheEntry:
        with self.global_cache_lock:
            entry = self._active_global_per_rtu_cache[coa].pop(type_ID)

            if remove_global_coa_if_done and self.is_global_command_free(type_ID):
                del self._active_global_coa_cache[type_ID]
            return entry

    def pop_param_mesage(self, coa: int, ioa: int) -> IECMsg:
        with self.param_cache_lock:
            return self._active_param_changes[coa].pop(ioa).msg

    def remove_global_entry(
        self, coa: int, type_ID: TypeID, remove_global_coa_if_done: bool = False
    ) -> None:
        _ = self.pop_global_entry(coa, type_ID, remove_global_coa_if_done)

    def is_dp_active(self, coa: int, ioa: int) -> bool:
        with self.dp_cache_lock:
            return ioa in self._active_dps[coa]

    def is_param_active(self, coa: int, ioa: int) -> bool:
        with self.param_cache_lock:
            return ioa in self._active_param_changes[coa]

    def global_send_and_active(self, coa: int, type_ID: TypeID) -> bool:
        with self.global_cache_lock:
            return type_ID in self._active_global_per_rtu_cache[coa] and self._active_global_per_rtu_cache[coa][
                type_ID
            ].status in (MsgStatus.RECEIVED_ACK, MsgStatus.SEND_NO_ACK)

    def is_global_confirmed(self, coa: int, type_ID: TypeID) -> bool:
        with self.global_cache_lock:
            return (
                    type_ID in self._active_global_per_rtu_cache[coa]
                    and self._active_global_per_rtu_cache[coa][type_ID].status == MsgStatus.RECEIVED_ACK
            )

    def is_global_term(self, coa: int, type_ID: TypeID) -> bool:
        """
        Only returns True if the command was used but not then deleted
         """
        with self.global_cache_lock:
            return (
                    type_ID in self._active_global_per_rtu_cache[coa]
                    and self._active_global_per_rtu_cache[coa][type_ID].status == MsgStatus.RECEIVED_TERM
            )

    def get_global_entry(self, coa: int, type_ID: TypeID) -> CacheEntry:
        with self.global_cache_lock:
            if type_ID not in self._active_global_per_rtu_cache[coa]:
                raise ValueError(f"type is not active for this coa {coa} {type_ID}")
            return self._active_global_per_rtu_cache[coa][type_ID]

    def get_interro_entry(self, coa: int) -> CacheEntry:
        return self.get_global_entry(coa, TypeID.C_IC_NA_1)

    def is_interro_send_and_active(self, coa: int) -> bool:
        return self.global_send_and_active(coa, TypeID.C_IC_NA_1)

    def _interro_confirmed(self, coa: int) -> bool:
        return self.is_global_confirmed(coa, TypeID.C_IC_NA_1)

    def is_interro_term(self, coa: int) -> bool:
        return self.is_global_term(coa, TypeID.C_IC_NA_1)

    def archive_as_confirmed(self, coa: int, ioa: int) -> None:
        with self.dp_cache_lock:
            if ioa not in self._active_dps[coa]:
                raise ValueError(f"Cannot set inactive dp as confirmed with coa:ioa {coa}:{ioa}")
            entry = self._active_dps[coa].pop(ioa)
            entry.set_confirmed()
            self._confirmed_dps[coa][ioa] = entry

    def store_new_active_dp(
        self, coa: int, ioa: int, entry: CacheEntry, error_on_overwrite: bool = True
    ) -> None:
        with self.dp_cache_lock:
            self._raise_if_invalid_initial_status(entry)
            assert coa in self._active_dps
            if error_on_overwrite and ioa in self._active_dps[coa]:
                raise CacheOverwriteError(
                    f"Asked to overwrite still active dp {coa}:{ioa} w/ " 
                    f"original entry {self._active_dps[coa][ioa]} to {entry}"
                )

            self._active_dps[coa][ioa] = entry

    def store_new_active_param(
            self, coa: int, ioa: int, entry: CacheEntry, error_on_overwrite: bool = True
    ) -> None:
        with self.param_cache_lock:
            self._raise_if_invalid_initial_status(entry)
            assert coa in self._active_param_changes
            if error_on_overwrite and ioa in self._active_param_changes[coa]:
                raise CacheOverwriteError(
                    f"Asked to overwrite still active dp {coa}:{ioa} w/ "
                    f"original entry {self._active_param_changes[coa][ioa]} to {entry}"
                )
            self._active_param_changes[coa][ioa] = entry

    def store_global_for_all_rtus(self, type_ID: TypeID, entry: CacheEntry) -> None:
        with self.global_cache_lock:
            self._raise_if_invalid_initial_status(entry)
            if entry.status == MsgStatus.WAITING_FOR_SEND:
                rtu_cache = self._queued_global_per_rtu_cache
                global_coa_cache = self._queued_global_coa_cache
            else:
                rtu_cache = self._active_global_per_rtu_cache
                global_coa_cache = self._active_global_coa_cache

            if type_ID in global_coa_cache:
                raise CacheOverwriteError(
                    f"Forbidden operation to overwrite still-active global-coa-cache with command {type_ID}."
                )

            global_coa_cache[type_ID] = entry
            for rtu_typed_cache in rtu_cache.values():
                if (
                    type_ID in rtu_typed_cache
                    # A subscriber may insert a message into the queue correctly
                    # but before it is send of, the MTU sends one with the same type of (especially C_CS)
                    # In this case, we need to allow the just send IEC104-Msg to be stored
                    #   in the cache anyhow.
                    # But never allow an ACTIVE entry to be overwritten.
                    and rtu_cache == self._active_global_per_rtu_cache
                    and rtu_typed_cache[type_ID].status != MsgStatus.RECEIVED_TERM
                ):
                    raise CacheOverwriteError(
                        "Overwriting for still active Global cache with target entry" f"{entry}"
                    )
                rtu_typed_cache[type_ID] = deepcopy(entry)

    def store_global(
        self, coa: int, type_ID: TypeID, entry: CacheEntry, error_on_overwrite: bool = True
    ) -> None:
        with self.global_cache_lock:
            self._raise_if_invalid_initial_status(entry)
            if coa == 65535:
                self.store_global_for_all_rtus(type_ID, entry)
            self._active_global_coa_cache[type_ID] = entry
            target_cache = (
                self._queued_global_per_rtu_cache if entry.status.WAITING_FOR_SEND else self._active_global_per_rtu_cache
            )
            if coa not in target_cache:
                raise KeyError(f"{coa} ({type(coa)}) not in cache: {target_cache}")

            if type_ID in target_cache and error_on_overwrite:
                raise CacheOverwriteError(
                    f"Asked to overwrite still global capable type {coa}:{type_ID} w/ "
                    f"entry {entry}"
                )

            target_cache[coa][type_ID] = entry

    def is_global_command_free(self, type_ID: TypeID) -> bool:
        # Only allow if no global-coa-compatible msg is active
        with self.global_cache_lock:
            for rtu_typed_cache in self._active_global_per_rtu_cache.values():
                if self._rtu_global_in_use_and_non_terminated(rtu_typed_cache, type_ID):
                    return False
            return True

    @staticmethod
    def _rtu_global_in_use_and_non_terminated(cache: Dict[TypeID, CacheEntry], type_ID: TypeID):
        return type_ID in cache and cache[type_ID].status != MsgStatus.RECEIVED_TERM

    def set_global_command_active(self, type_ID: TypeID, rtu_coa: int = -1) -> None:
        with self.global_cache_lock:
            if type_ID in self._active_global_coa_cache:
                raise CacheOverwriteError(f"Forbidden to overwrite global-COA command with type {type_ID}")
            orig_entry = self._queued_global_coa_cache.pop(type_ID)
            orig_entry.status = MsgStatus.SEND_NO_ACK
            if rtu_coa != -1:
                self.store_global(rtu_coa, type_ID, orig_entry)
            else:
                self.store_global_for_all_rtus(type_ID, orig_entry)

    def confirm_global_entry(self, coa: int, type_ID: TypeID) -> None:
        with self.global_cache_lock:
            if type_ID not in self._active_global_per_rtu_cache[coa]:
                raise ValueError(f"No global cmd known for this coa {coa} {type_ID}")

            orig_entry = self._active_global_per_rtu_cache[coa][type_ID]
            orig_entry.set_confirmed()
            if (type_ID in self._active_global_coa_cache
                and not [c[type_ID] for c in self._active_global_per_rtu_cache.values()
                            if type_ID in c and c[type_ID].status == MsgStatus.SEND_NO_ACK]
            ):
                e = self._active_global_coa_cache[type_ID]
                e.status = MsgStatus.RECEIVED_ACK
                e.ts = orig_entry.ts

    @staticmethod
    def _raise_if_invalid_initial_status(entry):
        """ Raises error if status != WAITING_FOR_SEND/ SEND_NO_ACK """
        if entry.status not in (MsgStatus.WAITING_FOR_SEND, MsgStatus.SEND_NO_ACK):
            raise IllegalCacheAccessError(
                f"Forbidden to insert entry to cache with status {entry.status}."
            )

    def __repr__(self):
        s = f"active_global_coa types: {sorted(self._active_global_coa_cache.keys())} "
        s += f"queued_global_coa types: {sorted(self._queued_global_coa_cache.keys())} "
        s += f"active IOAs {sorted([sorted(c.keys()) for c in self._active_dps.values()])} "
        s += f"confirmed IOAs {sorted([sorted(c.keys()) for c in self._confirmed_dps.values()])}"
        return s
